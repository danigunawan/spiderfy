# 🕸spiderfy🕸
**🕷 crawling , 🕸 scraping | 👓Playground [proof of concepts]**

**Url:** https://crawler.muradjs.now.sh/#

**Backend API:** https://spiderfy.herokuapp.com/swagger-ui.html

**Project abilities**

`⭐️ To retrieve all links from given website.`

`⭐️ To retrieve all images from given website.`

`⭐️To retrieve all metatags from given website.`

`⭐️To retrieve all sitemap nodes from given website.`

`⭐️To retrieve all links from given sitemap url.`

`⭐️To convert base64 to text (OCR function) from given snapshoot.`

`⭐️To retrieve all RSS Feeds from given RSS url.`

`⭐️To retrieve Turkey RSS News Feeds from RSS list.`

`⭐️To show 3500 user agent from static list.`

`⭐️To insert randomly selected user agent on http request according to user agents static list`

`⭐️To obtain only text without any html tags from given website.`

`⭐️To obtain only all html source code from given website.`

`⭐️To retrieve all links/link snapshoot on base64 format.`

`⭐️To obtain top 50 sites in Turkey from Alexa.`

`⭐️To obtain top 50 sites in Turkey from SimilarWeb.`



⚠️ This project is only proof of concepts, does not relate commercial use.


#                             TECH STACK
![techstack](https://raw.githubusercontent.com/fatihyildizli/spiderfy/master/tech.PNG)


![deployment](https://avatars3.githubusercontent.com/u/23211?size=30) Deployment plugins

- https://github.com/heroku/heroku-buildpack-google-chrome
- https://github.com/heroku/heroku-buildpack-chromedriver

