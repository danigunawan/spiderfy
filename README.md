# ğŸ•¸spiderfyğŸ•¸
**ğŸ•· crawling , ğŸ•¸ scraping | ğŸ‘“Playground [proof of concepts]**

**Url:** https://crawler.muradjs.now.sh/#

**Backend API:** https://spiderfy.herokuapp.com/swagger-ui.html

**Project abilities**

`â­ï¸ To retrieve all links from given website.`

`â­ï¸ To retrieve all images from given website.`

`â­ï¸To retrieve all metatags from given website.`

`â­ï¸To retrieve all sitemap nodes from given website.`

`â­ï¸To retrieve all links from given sitemap url.`

`â­ï¸To convert base64 to text (OCR function) from given snapshoot.`

`â­ï¸To retrieve all RSS Feeds from given RSS url.`

`â­ï¸To retrieve Turkey RSS News Feeds from RSS list.`

`â­ï¸To show 3500 user agent from static list.`

`â­ï¸To insert randomly selected user agent on http request according to user agents static list`

`â­ï¸To obtain only text without any html tags from given website.`

`â­ï¸To obtain only all html source code from given website.`

`â­ï¸To retrieve all links/link snapshoot on base64 format.`

`â­ï¸To obtain top 50 sites in Turkey from Alexa.`

`â­ï¸To obtain top 50 sites in Turkey from SimilarWeb.`



âš ï¸ This project is only proof of concepts, does not relate commercial use.


#                             TECH STACK
![techstack](https://raw.githubusercontent.com/fatihyildizli/spiderfy/master/tech.PNG)


![deployment](https://avatars3.githubusercontent.com/u/23211?size=30) Deployment plugins

- https://github.com/heroku/heroku-buildpack-google-chrome
- https://github.com/heroku/heroku-buildpack-chromedriver

